---
title: pytorch部署教程及相关理论理解
date: 2023-06-08 23:51:46
permalink: /pages/12334a/
categories:
  - 编程
  - 机器学习
  - 部署
tags:
  - pytorch
author:
  name: fovegage
  link: https://github.com/fovegage
---

## 原理

```

torchscript
onnx


使用 grpc
```

## pytroch

- batch_size: 每个batch加载多少个样本(默认: 1)
- **num_workers**: 用多少个子进程加载数据。0表示数据将在主进程中加载(默认: 0)
- local_rank: 进程内的 GPU 编号，非显式参数，这个一般由 torch.distributed.launch 内部指定。例如， rank = 3，local_rank = 0
  表示第 3 个进程内的第 1 块 GPU
- rank: rank=0为master节点，进程号
- World_size: 进程组中的进程数，可以认为是全局进程个数
- backend: 进程通信库, PyTorch 支持NCCL，GLOO，MPI
- group: 所有进程的子集，用于集体通信等
- checkpoint:是**一种用时间换显存的技术**，一般训练模式下，pytorch 每次运算后会保留一些中间变量用于求导，而使用checkpoint
  的函数，则不会保留中间变量，中间变量会在求导时再计算一次，因此减少了显存占用，跟tensorflow 的checkpoint 是完全不同的东西

![](https://obsidian-foveagge.oss-cn-beijing.aliyuncs.com/blog/Ptowgj.png)

## 资料

```
https://www.cnblogs.com/yanshw/p/16563257.html


# m1 libtorch
https://github.com/mlverse/libtorch-mac-m1

# [源码解析] PyTorch 分布式(4)------分布式应用基础概念
https://www.cnblogs.com/rossiXYZ/p/15546837.html
```