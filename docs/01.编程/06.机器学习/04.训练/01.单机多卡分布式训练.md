---
title: 单机多卡分布式训练
date: 2023-06-08 22:46:00
permalink: /pages/5ef19a/
categories:
  - 编程
  - 机器学习
  - 训练
tags:
  - 
author: 
  name: fovegage
  link: https://github.com/fovegage
---
### 机器选择
```

# batch_size = 128   需要 40G显存
# https://buy.cloud.tencent.com/price/gpu
128 per-GPU * 8 GPU


2023-04-26,02:59:47 | INFO | Rank 0 | Global Steps: 1940/3000 | Train Epoch: 2 [9400/10000 (94%)] | Loss: 0.002717 | Image2Text Acc: 100.00 | Text2Image Acc: 100.00 | Data Time: 0.075s | Batch Time: 0.242s | LR: 0.000015 | logit_scale: 4.592 | Global Batch Size: 10
```
### 资源利用
```
## 合理使用资源
1.  将训练过后的模型日志和其他重要的文件保存到谷歌云盘，而不是本地的实例空间
2.  运行的代码必须支持“断点续传”能力，简单来说就是必须定义类似checkpoint功能的函数；假设我们一共需要训练40个epochs，在第30个epoch掉线了之后模型能够从第30个epoch开始训练而不是从头再来 
3.  仅在模型训练时开启GPU模式，在构建模型或其他非必要情况下使用None模式 
4.  在网络稳定的情况下开始训练，每隔一段时间查看一下训练的情况 
5.  注册多个免费的谷歌账号交替使用
```
### 文档
```
# # PyTorch 单机多卡分布式训练
https://blog.csdn.net/weixin_38842821/article/details/119361194
```